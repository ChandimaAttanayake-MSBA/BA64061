# -*- coding: utf-8 -*-
"""CreitCrFraud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfZ0AUT4e6Ql-JDIINmJHtA_Mr3a0ZM_
"""

# Financial Fraud Detection with Balanced Precision/Recall

## 1. Setup and Installation
!pip install tensorflow scikit-learn imbalanced-learn > /dev/null

# 2. Data Loading and Exploration
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load data
data_path = '/content/drive/MyDrive/Colab Notebooks/Fraud Detection/creditcard.csv'
df = pd.read_csv(data_path)

# Initial exploration
print(f"Dataset shape: {df.shape}")
print("\nClass distribution:")
print(df['Class'].value_counts(normalize=True))

# Visualize imbalance
plt.figure(figsize=(6,4))
df['Class'].value_counts().plot(kind='bar', color=['green', 'red'])
plt.title('Original Class Distribution')
plt.xticks([0,1], ['Legitimate', 'Fraud'], rotation=0)
plt.show()

#  3. Data Preprocessing
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split

# Normalize Amount and Time
scaler = RobustScaler()
df['Amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1,1))
df['Time'] = scaler.fit_transform(df['Time'].values.reshape(-1,1))

# Train-test split (before any resampling)
X = df.drop('Class', axis=1)
y = df['Class']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 4. Sequence Generation
def create_sequences(data, labels, seq_length=30, fraud_ratio=0.3):
    """
    Creates sequences with:
    - Minimum 30 timesteps for better patterns
    - Controlled fraud representation (30% by default)
    - Ensures fraud cases are properly isolated
    """
    fraud_indices = np.where(labels==1)[0]
    legit_indices = np.where(labels==0)[0]

    sequences, targets = [], []

    # 1. Add all available fraud sequences
    for i in fraud_indices:
        if i > seq_length and (i + 5) < len(data):  # Ensure isolation
            seq = data.iloc[i-seq_length:i]
            sequences.append(seq.values)
            targets.append(1)

    # 2. Calculate number of legitimate sequences needed
    n_fraud = len(targets)
    n_legit = int(n_fraud * (1-fraud_ratio)/fraud_ratio)

    # 3. Add legitimate sequences
    selected_indices = np.random.choice(legit_indices, min(n_legit, len(legit_indices)), replace=False)
    for i in selected_indices:
        if i > seq_length:
            seq = data.iloc[i-seq_length:i]
            sequences.append(seq.values)
            targets.append(0)

    return np.array(sequences), np.array(targets)

# Create optimized sequences
X_train_seq, y_train_seq = create_sequences(pd.DataFrame(X_train), y_train, fraud_ratio=0.3)
X_test_seq, y_test_seq = create_sequences(pd.DataFrame(X_test), y_test, fraud_ratio=0.3)

print(f"\nTraining sequences: {X_train_seq.shape}")
print(f"Test sequences: {X_test_seq.shape}")
print("Class balance in training sequences:")
print(pd.Series(y_train_seq).value_counts(normalize=True))

# 5. LSTM Model Implementation
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Attention, Concatenate
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf

def build_optimized_model(input_shape):
    inputs = Input(shape=input_shape)

    # Bidirectional LSTM
    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)
    x = Dropout(0.2)(x)

    # Self-attention mechanism using TensorFlow's built-in Attention
    query = Dense(64)(x)
    key = Dense(64)(x)
    value = Dense(64)(x)
    attention_output = Attention()([query, key, value])

    # Concatenate with LSTM output
    x = Concatenate()([x, attention_output])
    x = LSTM(64, return_sequences=False)(x)
    x = Dense(32, activation='relu')(x)
    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs, outputs)

    # Enhanced focal loss
    def focal_loss(y_true, y_pred, alpha=0.8, gamma=2.0):
        bce = tf.keras.losses.BinaryCrossentropy(reduction='none')(y_true, y_pred)
        pt = tf.exp(-bce)
        loss = alpha * tf.pow(1-pt, gamma) * bce
        return tf.reduce_mean(loss)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(0.0005),
        loss=focal_loss,
        metrics=[
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.AUC(name='auc')
        ]
    )
    return model

model = build_optimized_model(X_train_seq.shape[1:])
model.summary()

# 6. Model Training
early_stop = EarlyStopping(
    monitor='val_auc',
    patience=10,
    mode='max',
    baseline=0.8,
    restore_best_weights=True
)

history = model.fit(
    X_train_seq, y_train_seq,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stop],
    class_weight={0:1, 1:30},
    verbose=1
)

# 7. Threshold Optimization
from sklearn.metrics import precision_recall_curve, f1_score

# Get predicted probabilities
y_probs = model.predict(X_test_seq).ravel()

# Find optimal threshold
precision, recall, thresholds = precision_recall_curve(y_test_seq, y_probs)
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

print(f"\nOptimal Threshold: {optimal_threshold:.4f}")

# 8. Final Evaluation
# Evaluate with optimal threshold
y_pred = (y_probs > optimal_threshold).astype(int)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

print("\nClassification Report at Optimal Threshold:")
print(classification_report(y_test_seq, y_pred, target_names=['Legitimate', 'Fraud']))

# Confusion matrix
cm = confusion_matrix(y_test_seq, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Legit', 'Predicted Fraud'],
            yticklabels=['Actual Legit', 'Actual Fraud'])
plt.title('Confusion Matrix')
plt.show()

# 9. Training History Visualization
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Curves')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['recall'], label='Train Recall')
plt.plot(history.history['val_recall'], label='Validation Recall')
plt.title('Recall Curves')
plt.legend()
plt.show()